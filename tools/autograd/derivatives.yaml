# Defines derivative formulas and Python signatures of methods on Variable

- name: __and__
- name: __iand__
- name: __ilshift__
- name: __ior__
- name: __irshift__
- name: __ixor__
- name: __lshift__
- name: __or__
- name: __rshift__
- name: __xor__

- name: abs(Tensor self)
  self: grad * self.sign()

- name: acos(Tensor self)
  self: grad * -((-self * self + 1).sqrt().reciprocal())

- name: add(Tensor self, Scalar other, *, Scalar alpha=1)
  self: grad

- name: add(Tensor self, Tensor other, *, Scalar alpha=1)
  aten: add(self, alpha, other)
  self: grad
  other: grad * alpha

- name: addbmm(Tensor self, Tensor batch1, Tensor batch2, *, Scalar beta=1, Scalar alpha=1)
  aten: addbmm(beta, self, alpha, batch1, batch2)
  self: grad * alpha
  batch1: grad.bmm(batch2.transpose(1, 2)) * beta
  batch2: batch1.transpose(1, 2).bmm(grad) * beta

- name: addcdiv(Tensor self, Tensor tensor1, Tensor tensor2, *, Scalar value=1)
  aten: addcdiv(self, value, tensor1, tensor2)
  self: grad * value
  tensor1: grad / tensor2 * value
  tensor2: -grad * tensor1 / (tensor2 * tensor2)

- name: addcmul(Tensor self, Tensor tensor1, Tensor tensor2, *, Scalar value=1)
  aten: addcmul(self, value, tensor1, tensor2)
  self: grad
  tensor1: grad * tensor2 * value
  tensor2: grad * tensor1 * value

- name: addmm(Tensor self, Tensor mat1, Tensor mat2, *, Scalar beta=1, Scalar alpha=1)
  aten: addmm(beta, self, alpha, mat1, mat2)
  self: addmm_self_backward(grad, beta)
  mat1: addmm_mat1_backward(grad, mat1, mat2, alpha)
  mat2: addmm_mat2_backward(grad, mat1, mat2, alpha)

- name: addmv(Tensor self, Tensor mat, Tensor vec, *, Scalar beta=1, Scalar alpha=1)
  aten: addmv(beta, self, alpha, mat, vec)
  self: grad * beta
  mat: grad.ger(vec) * alpha
  vec: mat.t().mv(grad) * alpha

- name: addr(Tensor self, Tensor vec1, Tensor vec2, *, Scalar beta=1, Scalar alpha=1)
  aten: addr(beta, self, alpha, vec1, vec2)
  self: grad * beta
  vec1: grad.mv(vec2) * alpha
  vec2: grad.t().mv(vec1) * alpha

- name: all  # fallthrough

- name: any  # fallthrough

- name: asin(Tensor self)
  self: grad * (-self * self + 1).sqrt().reciprocal()

- name: atan(Tensor self)
  self: grad * (self * self + 1).reciprocal()

- name: atan2(Tensor self, Tensor other)
  self: grad * other * ((self * self + other * other).reciprocal())
  other: grad * -self * ((self * self + other * other).reciprocal())

- name: baddbmm(Tensor self, Tensor batch1, Tensor batch2, *, Scalar beta=1, Scalar alpha=1)
  aten: baddbmm(beta, self, alpha, batch1, batch2)
  self: grad * beta
  batch1: grad.bmm(batch2.transpose(1, 2)) * alpha
  batch2: batch1.transpose(1, 2).bmm(grad) * alpha

- name: bmm(Tensor self, Tensor mat2)
  self: grad.bmm(mat2.transpose(1, 2))
  mat2: self.transpose(1, 2).bmm(grad)

- name: btrifact
- name: btrisolve

- name: cat(TensorList tensors, int64_t dim=0)
  tensors: cat_tensors_backward(grad, tensors.sizes(dim), dim)

- name: cauchy
- name: ceil

- name: clone(Tensor self)
  self: grad

- name: contiguous(Tensor self)
  self: grad

- name: cos(Tensor self)
  self: grad * -self.sin()

- name: cosh(Tensor self)
  self: grad * self.sinh()

- name: cross(Tensor self, Tensor other, int64_t dim=-1)
  self: other.cross(grad, dim)
  other: grad.cross(self, dim)

- name: cumprod

- name: cumsum(Tensor self, int64_t dim)
  self: cumsum_backward(grad, dim)

- name: data_ptr  # fallthrough

- name: diag(Tensor self, int64_t diagonal=0)
  self: grad.diag(diagonal)

- name: dist(Tensor self, Tensor other, Scalar p=2)
  self: norm_backward(grad, self - other, p)
  other: -norm_backward(grad, self - other, p)

- name: div(Tensor self, Scalar value)
  self: grad / value

- name: div(Tensor self, Tensor other)
  self: grad / other
  other: -grad * self / (other * other)

- name: dot(Tensor self, Tensor tensor)
  self: grad * tensor
  tensor: grad * self

- name: eig
- name: eq
- name: equal
- name: exp

- name: expand(Tensor self, IntList size)
  self: reduce_to(grad, self.sizes())

- name: eye
- name: fill
- name: floor
- name: fmod
- name: frac
- name: gather
- name: ge
- name: gels
- name: geometric
- name: geqrf

- name: ger(Tensor self, Tensor vec2)
  self: grad.mv(vec2)
  vec2: grad.t().mv(self)

- name: gesv

- name: get_device  # fallthrough

- name: gt
- name: histc
- name: index_add
- name: index_copy
- name: index_fill
- name: index_select
- name: inverse

- name: is_contiguous

- name: is_same_size  # fallthrough

- name: is_set_to  # fallthrough

- name: kthvalue
- name: le
- name: lerp
- name: lgamma
- name: linspace

- name: log(Tensor self)
  self: grad.div(self)

- name: log1p
- name: log_normal
- name: logspace
- name: lt
- name: masked_fill
- name: masked_scatter
- name: masked_select
- name: max
- name: mean
- name: median
- name: min

- name: mm(Tensor self, Tensor mat2)
  self: grad.mm(mat2.t())
  mat2: self.t().mm(grad)

- name: mode

- name: mul(Tensor self, Scalar value)
  self: grad * value

- name: mul(Tensor self, Tensor other)
  self: grad * other
  other: grad * self

- name: multinomial

- name: mv(Tensor self, Tensor vec)
  self: grad.ger(vec)
  vec: self.t().mv(grad)

- name: narrow(Tensor self, int64_t dimension, int64_t start, int64_t length)
  self: grad._unnarrow(dimension, start, self.size(dimension))

- name: _unnarrow(Tensor self, int64_t dimension, int64_t offset, int64_t dimSize)
  self: grad.narrow(dimension, offset, self.size(dimension))

- name: ne

- name: neg(Tensor self)
  self: grad.neg()

- name: nonzero

- name: norm(Tensor self, Scalar p=2)
  self: norm_backward(grad, self, p)

- name: norm(Tensor self, Scalar p, int64_t dim, bool keepdim=False)
  self: norm_backward(grad, self, p, dim, keepdim)

- name: numel
- name: ones
- name: orgqr
- name: ormqr
- name: potrf
- name: potri
- name: potrs

- name: pow(Tensor self, Scalar exponent)
  self: grad * exponent * self.pow(exponent.toDouble() - 1)

- name: pow(Tensor self, Tensor exponent)
  self: grad * exponent * self.pow(exponent - 1)
  exponent: grad * self.pow(exponent) * self.log()

- name: prod
- name: pstrf
- name: qr
- name: rand
- name: randn
- name: randperm
- name: range

- name: reciprocal(Tensor self)
  self: grad / -(self * self)

- name: remainder
- name: renorm
- name: resize
- name: resize_as
- name: round
- name: rsqrt
- name: scatter
- name: scatter_add
- name: select
- name: set
- name: sigmoid

- name: sign(Tensor self)
  self: zeros_like(grad)

- name: sin(Tensor self)
  self: grad * self.cos()

- name: sinh(Tensor self)
  self: grad * self.cosh()

- name: size
- name: sort

- name: sqrt(Tensor self)
  self: grad * self.pow(-0.5) / 2

- name: squeeze(Tensor self)
  self: unsqueeze_to(grad, self.sizes());

- name: squeeze(Tensor self, int64_t dim)
  self: grad.unsqueeze(dim)

- name: std

- name: storage_offset  # fallthrough

- name: stride  # fallthrough

- name: sub(Tensor self, Scalar other, *, Scalar alpha)
  self: grad

- name: sub(Tensor self, Tensor other, *, Scalar alpha)
  aten: sub(self, alpha, other)
  self: grad
  other: -grad * alpha

- name: sum(Tensor self)
  self: grad.expand(self.sizes())

- name: sum(Tensor self, int64_t dim, bool keepdim=False)
  self: sum_backward(grad, self.sizes(), dim, keepdim)

- name: svd
- name: symeig

- name: t(Tensor self)
  self: grad.t()

- name: tan
- name: tanh

- name: tensor  # fallthrough

- name: topk
- name: trace

- name: transpose(Tensor self, int64_t dim0, int64_t dim1)
  self: grad.transpose(dim0, dim1)

- name: tril(Tensor self, int64_t diagonal=0)
  self: grad.tril(diagonal)

- name: triu(Tensor self, int64_t diagonal=0)
  self: grad.triu(diagonal)

- name: trtrs
- name: trunc
- name: unfold
- name: uniform

- name: unsqueeze(Tensor self, int64_t dim)
  self: grad.squeeze(dim)

- name: var
- name: view
- name: zero
- name: zeros
